{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 840,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023823704586063133,
      "grad_norm": 7.423227787017822,
      "learning_rate": 3.5e-06,
      "loss": 10.5504,
      "step": 10
    },
    {
      "epoch": 0.047647409172126266,
      "grad_norm": 6.435707092285156,
      "learning_rate": 8.5e-06,
      "loss": 10.1658,
      "step": 20
    },
    {
      "epoch": 0.0714711137581894,
      "grad_norm": 11.882756233215332,
      "learning_rate": 9.914634146341463e-06,
      "loss": 10.3022,
      "step": 30
    },
    {
      "epoch": 0.09529481834425253,
      "grad_norm": 12.10744571685791,
      "learning_rate": 9.79268292682927e-06,
      "loss": 10.0419,
      "step": 40
    },
    {
      "epoch": 0.11911852293031566,
      "grad_norm": 8.399367332458496,
      "learning_rate": 9.682926829268292e-06,
      "loss": 9.5624,
      "step": 50
    },
    {
      "epoch": 0.1429422275163788,
      "grad_norm": 10.416719436645508,
      "learning_rate": 9.560975609756098e-06,
      "loss": 10.0914,
      "step": 60
    },
    {
      "epoch": 0.16676593210244192,
      "grad_norm": 6.157580852508545,
      "learning_rate": 9.439024390243903e-06,
      "loss": 9.8501,
      "step": 70
    },
    {
      "epoch": 0.19058963668850507,
      "grad_norm": 10.694074630737305,
      "learning_rate": 9.317073170731709e-06,
      "loss": 9.3136,
      "step": 80
    },
    {
      "epoch": 0.2144133412745682,
      "grad_norm": 9.208954811096191,
      "learning_rate": 9.195121951219513e-06,
      "loss": 9.8352,
      "step": 90
    },
    {
      "epoch": 0.23823704586063132,
      "grad_norm": 11.09457015991211,
      "learning_rate": 9.073170731707319e-06,
      "loss": 9.1513,
      "step": 100
    },
    {
      "epoch": 0.26206075044669447,
      "grad_norm": 15.121454238891602,
      "learning_rate": 8.951219512195123e-06,
      "loss": 8.9443,
      "step": 110
    },
    {
      "epoch": 0.2858844550327576,
      "grad_norm": 9.785090446472168,
      "learning_rate": 8.829268292682927e-06,
      "loss": 9.0759,
      "step": 120
    },
    {
      "epoch": 0.3097081596188207,
      "grad_norm": 9.994900703430176,
      "learning_rate": 8.707317073170733e-06,
      "loss": 8.7912,
      "step": 130
    },
    {
      "epoch": 0.33353186420488384,
      "grad_norm": 25.13405990600586,
      "learning_rate": 8.585365853658537e-06,
      "loss": 7.8504,
      "step": 140
    },
    {
      "epoch": 0.357355568790947,
      "grad_norm": 28.0319881439209,
      "learning_rate": 8.463414634146342e-06,
      "loss": 8.0682,
      "step": 150
    },
    {
      "epoch": 0.38117927337701013,
      "grad_norm": 10.695918083190918,
      "learning_rate": 8.341463414634147e-06,
      "loss": 7.3367,
      "step": 160
    },
    {
      "epoch": 0.4050029779630733,
      "grad_norm": 10.318818092346191,
      "learning_rate": 8.219512195121952e-06,
      "loss": 7.8635,
      "step": 170
    },
    {
      "epoch": 0.4288266825491364,
      "grad_norm": 30.068132400512695,
      "learning_rate": 8.097560975609758e-06,
      "loss": 7.6836,
      "step": 180
    },
    {
      "epoch": 0.4526503871351995,
      "grad_norm": 17.384653091430664,
      "learning_rate": 7.975609756097562e-06,
      "loss": 7.1457,
      "step": 190
    },
    {
      "epoch": 0.47647409172126265,
      "grad_norm": 15.164475440979004,
      "learning_rate": 7.853658536585366e-06,
      "loss": 7.1499,
      "step": 200
    },
    {
      "epoch": 0.5002977963073257,
      "grad_norm": 15.341960906982422,
      "learning_rate": 7.743902439024391e-06,
      "loss": 6.9668,
      "step": 210
    },
    {
      "epoch": 0.5241215008933889,
      "grad_norm": 22.337377548217773,
      "learning_rate": 7.621951219512196e-06,
      "loss": 6.4608,
      "step": 220
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 17.41109848022461,
      "learning_rate": 7.500000000000001e-06,
      "loss": 5.7371,
      "step": 230
    },
    {
      "epoch": 0.5717689100655152,
      "grad_norm": 22.12906837463379,
      "learning_rate": 7.378048780487805e-06,
      "loss": 5.7586,
      "step": 240
    },
    {
      "epoch": 0.5955926146515783,
      "grad_norm": 15.410931587219238,
      "learning_rate": 7.25609756097561e-06,
      "loss": 4.9831,
      "step": 250
    },
    {
      "epoch": 0.6194163192376414,
      "grad_norm": 32.561195373535156,
      "learning_rate": 7.1341463414634146e-06,
      "loss": 4.131,
      "step": 260
    },
    {
      "epoch": 0.6432400238237046,
      "grad_norm": 17.926294326782227,
      "learning_rate": 7.01219512195122e-06,
      "loss": 4.1448,
      "step": 270
    },
    {
      "epoch": 0.6670637284097677,
      "grad_norm": 14.723697662353516,
      "learning_rate": 6.890243902439025e-06,
      "loss": 4.0982,
      "step": 280
    },
    {
      "epoch": 0.6908874329958309,
      "grad_norm": 15.755074501037598,
      "learning_rate": 6.768292682926831e-06,
      "loss": 4.0552,
      "step": 290
    },
    {
      "epoch": 0.714711137581894,
      "grad_norm": 18.730749130249023,
      "learning_rate": 6.646341463414635e-06,
      "loss": 4.0568,
      "step": 300
    },
    {
      "epoch": 0.7385348421679571,
      "grad_norm": 24.073257446289062,
      "learning_rate": 6.52439024390244e-06,
      "loss": 3.2491,
      "step": 310
    },
    {
      "epoch": 0.7623585467540203,
      "grad_norm": 25.789630889892578,
      "learning_rate": 6.402439024390244e-06,
      "loss": 3.3741,
      "step": 320
    },
    {
      "epoch": 0.7861822513400833,
      "grad_norm": 21.692651748657227,
      "learning_rate": 6.280487804878049e-06,
      "loss": 3.0411,
      "step": 330
    },
    {
      "epoch": 0.8100059559261465,
      "grad_norm": 32.340675354003906,
      "learning_rate": 6.158536585365854e-06,
      "loss": 2.4155,
      "step": 340
    },
    {
      "epoch": 0.8338296605122096,
      "grad_norm": 26.538570404052734,
      "learning_rate": 6.0365853658536585e-06,
      "loss": 2.3157,
      "step": 350
    },
    {
      "epoch": 0.8576533650982728,
      "grad_norm": 22.454456329345703,
      "learning_rate": 5.914634146341464e-06,
      "loss": 2.2394,
      "step": 360
    },
    {
      "epoch": 0.8814770696843359,
      "grad_norm": 22.01784896850586,
      "learning_rate": 5.792682926829269e-06,
      "loss": 1.9642,
      "step": 370
    },
    {
      "epoch": 0.905300774270399,
      "grad_norm": 15.978975296020508,
      "learning_rate": 5.670731707317073e-06,
      "loss": 1.7529,
      "step": 380
    },
    {
      "epoch": 0.9291244788564622,
      "grad_norm": 22.012338638305664,
      "learning_rate": 5.548780487804879e-06,
      "loss": 1.2069,
      "step": 390
    },
    {
      "epoch": 0.9529481834425253,
      "grad_norm": 22.737707138061523,
      "learning_rate": 5.426829268292684e-06,
      "loss": 1.4191,
      "step": 400
    },
    {
      "epoch": 0.9767718880285885,
      "grad_norm": 24.888832092285156,
      "learning_rate": 5.304878048780488e-06,
      "loss": 0.9757,
      "step": 410
    },
    {
      "epoch": 1.0,
      "grad_norm": 28.376623153686523,
      "learning_rate": 5.182926829268293e-06,
      "loss": 0.9708,
      "step": 420
    },
    {
      "epoch": 1.0238237045860632,
      "grad_norm": 5.954780101776123,
      "learning_rate": 5.060975609756098e-06,
      "loss": 0.8422,
      "step": 430
    },
    {
      "epoch": 1.0476474091721262,
      "grad_norm": 19.173503875732422,
      "learning_rate": 4.9390243902439025e-06,
      "loss": 0.7892,
      "step": 440
    },
    {
      "epoch": 1.0714711137581894,
      "grad_norm": 25.12030792236328,
      "learning_rate": 4.817073170731708e-06,
      "loss": 0.9715,
      "step": 450
    },
    {
      "epoch": 1.0952948183442526,
      "grad_norm": 15.018868446350098,
      "learning_rate": 4.695121951219513e-06,
      "loss": 0.7618,
      "step": 460
    },
    {
      "epoch": 1.1191185229303158,
      "grad_norm": 15.034388542175293,
      "learning_rate": 4.573170731707318e-06,
      "loss": 0.7546,
      "step": 470
    },
    {
      "epoch": 1.1429422275163788,
      "grad_norm": 11.470956802368164,
      "learning_rate": 4.451219512195122e-06,
      "loss": 0.6601,
      "step": 480
    },
    {
      "epoch": 1.166765932102442,
      "grad_norm": 8.52535629272461,
      "learning_rate": 4.329268292682927e-06,
      "loss": 0.4758,
      "step": 490
    },
    {
      "epoch": 1.1905896366885051,
      "grad_norm": 13.602680206298828,
      "learning_rate": 4.207317073170732e-06,
      "loss": 0.5328,
      "step": 500
    },
    {
      "epoch": 1.2144133412745681,
      "grad_norm": 8.902276039123535,
      "learning_rate": 4.085365853658536e-06,
      "loss": 0.5338,
      "step": 510
    },
    {
      "epoch": 1.2382370458606313,
      "grad_norm": 13.959898948669434,
      "learning_rate": 3.963414634146342e-06,
      "loss": 0.6668,
      "step": 520
    },
    {
      "epoch": 1.2620607504466945,
      "grad_norm": 18.724742889404297,
      "learning_rate": 3.8414634146341465e-06,
      "loss": 0.6496,
      "step": 530
    },
    {
      "epoch": 1.2858844550327575,
      "grad_norm": 6.604304790496826,
      "learning_rate": 3.7195121951219516e-06,
      "loss": 0.5473,
      "step": 540
    },
    {
      "epoch": 1.3097081596188207,
      "grad_norm": 21.63248634338379,
      "learning_rate": 3.5975609756097562e-06,
      "loss": 0.6484,
      "step": 550
    },
    {
      "epoch": 1.333531864204884,
      "grad_norm": 14.013975143432617,
      "learning_rate": 3.475609756097561e-06,
      "loss": 0.6114,
      "step": 560
    },
    {
      "epoch": 1.3573555687909469,
      "grad_norm": 14.594644546508789,
      "learning_rate": 3.3536585365853664e-06,
      "loss": 0.63,
      "step": 570
    },
    {
      "epoch": 1.38117927337701,
      "grad_norm": 12.278104782104492,
      "learning_rate": 3.231707317073171e-06,
      "loss": 0.4631,
      "step": 580
    },
    {
      "epoch": 1.4050029779630733,
      "grad_norm": 8.644553184509277,
      "learning_rate": 3.1097560975609757e-06,
      "loss": 0.4945,
      "step": 590
    },
    {
      "epoch": 1.4288266825491365,
      "grad_norm": 17.793964385986328,
      "learning_rate": 2.9878048780487808e-06,
      "loss": 0.4957,
      "step": 600
    },
    {
      "epoch": 1.4526503871351995,
      "grad_norm": 20.34783363342285,
      "learning_rate": 2.8658536585365854e-06,
      "loss": 0.4876,
      "step": 610
    },
    {
      "epoch": 1.4764740917212626,
      "grad_norm": 19.833885192871094,
      "learning_rate": 2.7439024390243905e-06,
      "loss": 0.5589,
      "step": 620
    },
    {
      "epoch": 1.5002977963073256,
      "grad_norm": 10.434209823608398,
      "learning_rate": 2.6219512195121956e-06,
      "loss": 0.4555,
      "step": 630
    },
    {
      "epoch": 1.524121500893389,
      "grad_norm": 15.405384063720703,
      "learning_rate": 2.5e-06,
      "loss": 0.4568,
      "step": 640
    },
    {
      "epoch": 1.547945205479452,
      "grad_norm": 15.89803409576416,
      "learning_rate": 2.378048780487805e-06,
      "loss": 0.3855,
      "step": 650
    },
    {
      "epoch": 1.5717689100655152,
      "grad_norm": 9.676521301269531,
      "learning_rate": 2.25609756097561e-06,
      "loss": 0.428,
      "step": 660
    },
    {
      "epoch": 1.5955926146515784,
      "grad_norm": 15.574504852294922,
      "learning_rate": 2.1341463414634146e-06,
      "loss": 0.5297,
      "step": 670
    },
    {
      "epoch": 1.6194163192376414,
      "grad_norm": 6.479332447052002,
      "learning_rate": 2.0121951219512197e-06,
      "loss": 0.4244,
      "step": 680
    },
    {
      "epoch": 1.6432400238237046,
      "grad_norm": 35.17544174194336,
      "learning_rate": 1.8902439024390245e-06,
      "loss": 0.4925,
      "step": 690
    },
    {
      "epoch": 1.6670637284097678,
      "grad_norm": 17.046720504760742,
      "learning_rate": 1.7682926829268294e-06,
      "loss": 0.4417,
      "step": 700
    },
    {
      "epoch": 1.6908874329958308,
      "grad_norm": 12.172698020935059,
      "learning_rate": 1.6463414634146345e-06,
      "loss": 0.5313,
      "step": 710
    },
    {
      "epoch": 1.714711137581894,
      "grad_norm": 15.991145133972168,
      "learning_rate": 1.5243902439024391e-06,
      "loss": 0.5259,
      "step": 720
    },
    {
      "epoch": 1.7385348421679572,
      "grad_norm": 12.007672309875488,
      "learning_rate": 1.4024390243902442e-06,
      "loss": 0.4258,
      "step": 730
    },
    {
      "epoch": 1.7623585467540201,
      "grad_norm": 17.12905502319336,
      "learning_rate": 1.2804878048780488e-06,
      "loss": 0.3672,
      "step": 740
    },
    {
      "epoch": 1.7861822513400833,
      "grad_norm": 16.82354736328125,
      "learning_rate": 1.158536585365854e-06,
      "loss": 0.4046,
      "step": 750
    },
    {
      "epoch": 1.8100059559261465,
      "grad_norm": 4.91335916519165,
      "learning_rate": 1.0365853658536586e-06,
      "loss": 0.3569,
      "step": 760
    },
    {
      "epoch": 1.8338296605122095,
      "grad_norm": 13.152830123901367,
      "learning_rate": 9.146341463414634e-07,
      "loss": 0.5432,
      "step": 770
    },
    {
      "epoch": 1.857653365098273,
      "grad_norm": 17.747966766357422,
      "learning_rate": 7.926829268292684e-07,
      "loss": 0.4919,
      "step": 780
    },
    {
      "epoch": 1.881477069684336,
      "grad_norm": 22.752017974853516,
      "learning_rate": 6.707317073170733e-07,
      "loss": 0.5057,
      "step": 790
    },
    {
      "epoch": 1.905300774270399,
      "grad_norm": 12.210700988769531,
      "learning_rate": 5.487804878048781e-07,
      "loss": 0.5068,
      "step": 800
    },
    {
      "epoch": 1.9291244788564623,
      "grad_norm": 4.022358417510986,
      "learning_rate": 4.26829268292683e-07,
      "loss": 0.6596,
      "step": 810
    },
    {
      "epoch": 1.9529481834425253,
      "grad_norm": 4.316075325012207,
      "learning_rate": 3.0487804878048784e-07,
      "loss": 0.2953,
      "step": 820
    },
    {
      "epoch": 1.9767718880285885,
      "grad_norm": 5.567054748535156,
      "learning_rate": 1.8292682926829268e-07,
      "loss": 0.3598,
      "step": 830
    },
    {
      "epoch": 2.0,
      "grad_norm": 10.61594295501709,
      "learning_rate": 6.097560975609757e-08,
      "loss": 0.3898,
      "step": 840
    }
  ],
  "logging_steps": 10,
  "max_steps": 840,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.806435192327373e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
