{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b487835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 3358 training samples.\n",
      "Using device: cuda\n",
      " Loading model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Model loaded on device: cuda\n",
      "Loading dataset...\n",
      "Loaded 720 samples for inference.\n",
      "Saved test split to test_split.csv for evaluation consistency.\n",
      "Starting zero-shot / few-shot inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/3358 [00:00<?, ? examples/s]\n",
      "Map:  30%|##9       | 1000/3358 [00:00<00:00, 6314.76 examples/s]\n",
      "Map:  60%|#####9    | 2000/3358 [00:00<00:00, 7038.65 examples/s]\n",
      "Map:  89%|########9 | 3000/3358 [00:00<00:00, 7225.31 examples/s]\n",
      "Map: 100%|##########| 3358/3358 [00:00<00:00, 6821.91 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/720 [00:00<?, ? examples/s]\n",
      "Map: 100%|##########| 720/720 [00:00<00:00, 7241.08 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/720 [00:00<?, ? examples/s]\n",
      "Map: 100%|##########| 720/720 [00:00<00:00, 3737.89 examples/s]\n",
      "Map: 100%|##########| 720/720 [00:00<00:00, 3699.47 examples/s]\n",
      "\n",
      "  0%|          | 0/720 [00:00<?, ?it/s]\n",
      "  0%|          | 0/720 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 58, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 43, in main\n",
      "    inference.run_inference()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\zero_shot_inference.py\", line 135, in run_inference\n",
      "    logger(prompt)\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\utils.py\", line 17, in __call__\n",
      "    print(message)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 0-1: character maps to <undefined>\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 58, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 42, in main\n",
      "    inference = ZS_Inference(args)\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\zero_shot_inference.py\", line 41, in __init__\n",
      "    self.load_data()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\zero_shot_inference.py\", line 81, in load_data\n",
      "    print(\"Loading dataset...\")\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f4d8' in position 0: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 3358 training samples.\n",
      "Using device: cuda\n",
      " Loading model: Qwen/Qwen2.5-1.5B-Instruct\n",
      " Model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "!python run_experiments.py --model \"Qwen/Qwen2.5-1.5B-Instruct\" --csv_path \"ground_truth.csv\" --prompt_style 1 --shots 0 --save_path \"./zs_preds\" --call_limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77530f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface_hub[hf_xet]) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2021.10.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mkahmed\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mkahmed\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "\n",
      "Fetching 2 files:   0%|          | 0/2 [24:54<?, ?it/s]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\http\\client.py\", line 462, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\http\\client.py\", line 506, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 58, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\run_experiments.py\", line 42, in main\n",
      "    inference = ZS_Inference(args)\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\zero_shot_inference.py\", line 40, in __init__\n",
      "    self.load_model()\n",
      "  File \"C:\\Users\\mkahmed\\Desktop\\miniproject\\zero_shot_inference.py\", line 62, in load_model\n",
      "    self.model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 597, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4900, in from_pretrained\n",
      "    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1200, in _get_resolved_checkpoint_files\n",
      "    checkpoint_files, sharded_metadata = get_checkpoint_shard_files(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 1084, in get_checkpoint_shard_files\n",
      "    cached_filenames = cached_files(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 567, in cached_files\n",
      "    raise e\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\", line 494, in cached_files\n",
      "    snapshot_download(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\_snapshot_download.py\", line 332, in snapshot_download\n",
      "    thread_map(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\tqdm\\contrib\\concurrent.py\", line 69, in thread_map\n",
      "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\tqdm\\contrib\\concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\concurrent\\futures\\_base.py\", line 608, in result_iterator\n",
      "    yield fs.pop().result()\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\concurrent\\futures\\_base.py\", line 445, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\concurrent\\futures\\_base.py\", line 390, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\concurrent\\futures\\thread.py\", line 52, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\_snapshot_download.py\", line 306, in _inner_hf_hub_download\n",
      "    return hf_hub_download(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1007, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1168, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1735, in _download_to_tmp_and_move\n",
      "    http_get(\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 493, in http_get\n",
      "    for chunk in r.iter_content(chunk_size=constants.DOWNLOAD_CHUNK_SIZE):\n",
      "  File \"C:\\Users\\mkahmed\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 822, in generate\n",
      "    raise ChunkedEncodingError(e)\n",
      "requests.exceptions.ChunkedEncodingError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 3358 training samples.\n",
      "Using device: cuda\n",
      " Loading model: tiiuae/falcon-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3540252",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.57.0\n",
    "!pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6194c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a60ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate base  # or your GPU environment name\n",
    "python -m ipykernel install --user --name gpu-env --display-name \"Python (GPU)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd198bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6413eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648787c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.device_count())  # number of GPUs\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pillow>=10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)          # shows which CUDA PyTorch is using\n",
    "print(torch.cuda.is_available())   # should now be True\n",
    "print(torch.cuda.device_count())   # should be >=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b849c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name gpu-env --display-name \"Python (GPU)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69307294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
